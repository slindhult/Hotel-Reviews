{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold, GridSearchCV, cross_val_score\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from: https://www.kaggle.com/jiashenliu/515k-hotel-reviews-data-in-europe\n",
    "data = pd.read_csv('Hotel_Reviews.csv')\n",
    "\n",
    "# remove rows with no reviews\n",
    "data = data[(data['Negative_Review'] != 'No Negative') | (data['Positive_Review'] != 'No Positive')]\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many unqiue in each column\n",
    "for i in data.columns:\n",
    "    print(i, len(data[i].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing if there was a difference between ratings from europeans and non-europeans\n",
    "europe = pd.read_html('https://en.wikipedia.org/wiki/Ethnic_groups_in_Europe')\n",
    "\n",
    "euro = list(europe[4]['Country'])\n",
    "for idx, i in enumerate(euro):\n",
    "    if not i.isalpha():\n",
    "        euro[idx] = i[:-3]\n",
    "    if i =='United King':\n",
    "        euro[idx] = 'United Kingdom'\n",
    "euro[6] = 'Bosnia and Herzegovina'\n",
    "\n",
    "euro_or_no = list(data['Reviewer_Nationality'])\n",
    "for idx, i in enumerate(euro_or_no):\n",
    "    euro_or_no[idx] = i[1:-1]\n",
    "for idx,i in enumerate(euro_or_no):\n",
    "\n",
    "    if i in list(euro):\n",
    "        euro_or_no[idx] = 'European'\n",
    "    else:\n",
    "        euro_or_no[idx] = 'Not European'\n",
    "\n",
    "data['Reviewer_Nationality'] = euro_or_no\n",
    "euros = data.copy()\n",
    "euros.drop(['Hotel_Address', 'Additional_Number_of_Scoring', 'Review_Date',\n",
    "        'Hotel_Name',\n",
    "       'Negative_Review', \n",
    "       'Total_Number_of_Reviews', 'Positive_Review',\n",
    "       'Total_Number_of_Reviews_Reviewer_Has_Given', 'Tags',\n",
    "       'days_since_review', 'lat', 'lng'], axis=1, inplace=True)\n",
    "euros.Reviewer_Nationality.unique()\n",
    "euros.groupby('Reviewer_Nationality').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_address(address, cities):\n",
    "    for city in cities:\n",
    "        if city in address:\n",
    "            return city\n",
    "cities = ['Amsterdam', 'Vienna', 'Milan', 'Barcelona', 'Paris', 'London']\n",
    "data['city'] = data['Hotel_Address'].apply(lambda x: city_address(x, cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking monthly trend, consistent scoring\n",
    "monthdf = data[['Review_Date','Average_Score','Reviewer_Score']]\n",
    "monthdf['Review_Date'] = pd.to_datetime(monthdf['Review_Date'])\n",
    "monthdf['Review_Date'] = monthdf['Review_Date'].apply(lambda x: x.strftime('%m'))\n",
    "monthdf['City'] = data['city']\n",
    "monthdf = monthdf.groupby(['Review_Date', 'City']).agg({'Average_Score':'mean', 'Reviewer_Score':'mean'})\n",
    "monthdf.reset_index(inplace=True)\n",
    "monthdf.sort_values('City')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cities:\n",
    "    tempdf = monthdf[monthdf['City']==i]\n",
    "    plt.plot(tempdf['Review_Date'], tempdf['Reviewer_Score'], label=i)\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Average Rating')\n",
    "    plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data['Total_Number_of_Reviews_Reviewer_Has_Given'],data['Reviewer_Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data['Average_Score'],data['Reviewer_Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data['Reviewer_Score'], bins=50);\n",
    "plt.title('Distribution of Reviewer Scores')\n",
    "plt.xlabel('Reviewer Score')\n",
    "plt.ylabel('Occurrences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unwanted columns\n",
    "data.drop(['Hotel_Name','Hotel_Address','Review_Date','days_since_review', 'lat', 'lng'],  axis=1, inplace=True)\n",
    "data['Average_Score'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correcting tags from 55,000 to a few - started as a list of strings that looked like lists of strings\n",
    "tags = data.Tags\n",
    "\n",
    "import ast\n",
    "new_tags = []\n",
    "for i in tags:\n",
    "    new_tags.append(ast.literal_eval(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check most common tags\n",
    "from collections import Counter\n",
    "list_for_counting = []\n",
    "for i in new_tags:\n",
    "    for t in i:\n",
    "        list_for_counting.append(t)\n",
    "    \n",
    "c = Counter(list_for_counting)\n",
    "c.most_common(20)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering tags to reduce columns\n",
    "for index, i in enumerate(new_tags):\n",
    "    for idx, t in enumerate(i):\n",
    "        if t not in ['Stayed 1-2 nights','Stayed 3-4 nights', 'Stayed 5+ nights', 'Fancy', 'Budget', 'Medium', 'High', ' Business trip ', ' Solo traveler ', ' Leisure trip ', ' Couple ', ' Group ', ' Family with young children ', ' Family with older children ']:\n",
    "            if t in [' Stayed 1 night ',' Stayed 2 nights ']:\n",
    "                new_tags[index][idx] = 'Stayed 1-2 nights'\n",
    "            if t in [' Stayed 3 nights ',' Stayed 4 nights ']:\n",
    "                new_tags[index][idx] = 'Stayed 3-4 nights'\n",
    "            if t in [' Stayed 5 nights ',' Stayed 6 nights ', ' Stayed 7 nights ', ' Stayed 8 nights ', ' Stayed 9 nights ', ' Stayed 10 nights ',  ' Stayed 11 nights ',\n",
    "                 ' Stayed 12 nights ', ' Stayed 13 nights ', ' Stayed 14 nights ', ' Stayed 15 nights ', ' Stayed 16 nights ', ' Stayed 17 nights ',' Stayed 18 nights ', ' Stayed 19 nights ', ' Stayed 20 nights ',\n",
    "                 ' Stayed 21 nights ', ' Stayed 22 nights ', ' Stayed 23 nights ', ' Stayed 24 nights ', ' Stayed 25 nights ', ' Stayed 26 nights ',\n",
    "                 ' Stayed 27 nights ', ' Stayed 28 nights ', ' Stayed 29 nights ', ' Stayed 30 nights ', ' Stayed 31 nights ',]:\n",
    "                new_tags[index][idx] = 'Stayed 5+ nights'\n",
    "            if 'Luxury' in t or 'VIP' in t or 'Executive' in t or 'Ambassador' in t or 'Royal' in t or 'Penthouse' in t or 'Suite' in t or 'Duplex' in t or 'Presidential' in t or 'Apartment' in t or 'Apartement' in t:\n",
    "                new_tags[index][idx] = 'Fancy'\n",
    "            if 'Superior' in t or 'Premium' in t or 'Prestige' in t or 'Premiere' in t or 'Privilege' in t or 'Deluxe' in t or 'Premier' in t or 'Club' in t or 'View' in t or 'Art' in t or 'Fabulous' in t or 'Wonderful' in t or 'Loft' in t or 'Eiffel' in t or 'Spa' in t or 'King' in t:\n",
    "                new_tags[index][idx] = 'High'\n",
    "            if 'Standard' in t or 'Budget' in t or 'Small' in t or 'Economy' in t or 'Basic' in t or 'Bunk Bed' in t or 'Interior' in t or 'Special Offer' in t or 'Triple' in t or 'Quadruple' in t or 'Quintuple' in t or 'Sextuple' in t or 'Junior' in t or 'Twin' in t or 'Mini' in t or 'Check In' in t or 'Check in' in t or'Solo' in t or 'Camper' in t or 'Rooms' in t or 'Interconnecting' in t or 'FAMILY' in t or 'Atrium' in t or 'rooms' in t:\n",
    "                new_tags[index][idx] = 'Budget'\n",
    "            if 'Comfort' in t or 'Family' in t or 'Classic' in t or 'Large' in t or 'Double' in t or 'Cosy' in t or 'Single' in t or 'Connecting' in t or 'Queen' in t or 'Cozy' in t or 'Studio' in t or 'Adjacent' in t or 'Two' in t:\n",
    "                new_tags[index][idx] = 'Medium'\n",
    "            \n",
    "for index, i in enumerate(new_tags):\n",
    "    for idx, t in enumerate(i):            \n",
    "            if t not in ['Stayed 1-2 nights','Stayed 3-4 nights', 'Stayed 5+ nights', 'Fancy', 'Budget', 'Medium', 'High', ' Business trip ', ' Solo traveler ', ' Leisure trip ', ' Couple ', ' Group ', ' Family with young children ', ' Family with older children ']:\n",
    "                   new_tags[index][idx] = 'High'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = []\n",
    "for i in new_tags:\n",
    "    for t in i:\n",
    "        if t not in unique_tags:\n",
    "            unique_tags.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace tags with feature engineered tags\n",
    "data['New_Tags'] = new_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Onehotencode tags and drop the old columns\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "tagdf = pd.DataFrame(mlb.fit_transform(data.New_Tags),columns=mlb.classes_, index=data.index)\n",
    "\n",
    "\n",
    "data = data.join(tagdf)\n",
    "data = data.drop(['New_Tags'], axis=1)\n",
    "data.drop('Tags', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Reviewer_Nationality'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean = clean.to_csv('clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Negative_Review\"] = data[\"Negative_Review\"].apply(lambda x: str(x).replace(\"No Negative\", \" \"))\n",
    "data[\"Positive_Review\"] = data[\"Positive_Review\"].apply(lambda x: str(x).replace(\"No Positive\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = data[\"Negative_Review\"]\n",
    "pos = data[\"Positive_Review\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "neg_review_sentiment = []\n",
    "for i in neg:\n",
    "    review = TextBlob(i)\n",
    "    neg_review_sentiment.append(review.sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_review_sentiments = [i.polarity for i in neg_review_sentiment]\n",
    "data['neg_review_sentiment'] = neg_review_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_review_sentiment = []\n",
    "for i in pos:\n",
    "    review = TextBlob(i)\n",
    "    pos_review_sentiment.append(review.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_review_sentiments = [i.polarity for i in pos_review_sentiment]\n",
    "data['pos_review_sentiment'] = pos_review_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords and lemmatize reviews for nmf and most frequent word comparisons\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "data['Neg_Review_Clean'] = data['Negative_Review']\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered = []\n",
    "for i in data['Neg_Review_Clean']:\n",
    "    i = i.split()\n",
    "    filtered_sentence = [lem.lemmatize(w.lower()) for w in i if w not in stop_words]\n",
    "    filtered.append(' '.join(filtered_sentence))\n",
    "    \n",
    "data['Neg_Review_Clean'] = filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords and lemmatize reviews\n",
    "data['Pos_Review_Clean'] = data['Positive_Review']\n",
    "\n",
    "filteredpos = []\n",
    "for i in data['Pos_Review_Clean']:\n",
    "    i = i.split()\n",
    "    filtered_sentence = [lem.lemmatize(w) for w in i if w not in stop_words]\n",
    "    filteredpos.append(' '.join(filtered_sentence))\n",
    "    \n",
    "data['Pos_Review_Clean'] = filteredpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('cleanest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making same dataset using vader sentiment analysis for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanest = pd.read_csv('cleanest.csv')\n",
    "#cleanest.fillna(' ', inplace=True)\n",
    "#cleanest.drop(['Unnamed: 0','Neg_Review_Clean', 'Pos_Review_Clean'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "pos = cleanest['Positive_Review']\n",
    "pos_review_sentiment = []\n",
    "for i in pos:\n",
    "    snt = analyser.polarity_scores(i)\n",
    "    pos_review_sentiment.append(snt['compound'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = cleanest['Negative_Review']\n",
    "neg_review_sentiment = []\n",
    "for i in neg:\n",
    "    snt = analyser.polarity_scores(i)\n",
    "    neg_review_sentiment.append(snt['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanest['vader_pos_sent'] = pos_review_sentiment\n",
    "cleanest['vader_neg_sent'] = neg_review_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanest.drop(['neg_review_sentiment', 'pos_review_sentiment'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanest.drop(['Positive_Review', 'Negative_Review'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanest.to_csv('vader.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# has textblob sentiment\n",
    "cleanest = pd.read_csv('cleanest.csv')\n",
    "cleanest.fillna(' ', inplace=True)\n",
    "cleanest.drop(['Unnamed: 0', 'Negative_Review', 'Positive_Review', 'Neg_Review_Clean', 'Pos_Review_Clean'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop due to colinearity with total reviews\n",
    "cleanest.drop('Additional_Number_of_Scoring', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling with Textblob sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark - error if you always guessed the average (1.18)\n",
    "mean_absolute_error(cleanest['Average_Score'], cleanest['Reviewer_Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cleanest.pop('Reviewer_Score')\n",
    "X = cleanest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(min_samples_leaf = 30,\n",
    "                          max_depth=25,\n",
    "                          max_features=10,\n",
    "                          n_estimators=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Out of the box random forest with textblob sentiment analysis: 0.8860822932892579"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdbr = GradientBoostingRegressor(learning_rate=0.05,\n",
    "                                  loss='ls',\n",
    "                                 max_depth=15,\n",
    "                                  n_estimators=500,\n",
    "                                 min_samples_leaf=120,\n",
    "                                 max_features=15,\n",
    "                                  random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = gdbr.predict(X_test)\n",
    "mean_absolute_error(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Out of the box gradient boost with textblob analysis: 0.8735888232428165 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_train = xgb.DMatrix(X_train, label=y_train)\n",
    "xg_test = xgb.DMatrix(X_test, label=y_test)\n",
    "param = {}\n",
    "# I used gamma regression \n",
    "param['objective'] = 'reg:gamma'\n",
    "param['eta'] = 0.05\n",
    "param['max_depth'] =6\n",
    "param['silent'] = 0\n",
    "param['nthread'] = 4\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "num_round = 250\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist)\n",
    "pred = bst.predict(xg_test)\n",
    "mean_absolute_error(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Out of the box XGboost with textblob sentiment analysis: 0.8773096008942032"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling with original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanest.drop(['neg_review_sentiment', 'pos_review_sentiment'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cleanest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Out of the box random forest no sentiment analysis: 0.9172898922210799"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdbr.fit(X_train, y_train)\n",
    "y_hat = gdbr.predict(X_test)\n",
    "mean_absolute_error(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Out of the box gradient boost: 0.9180026868600099 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_train = xgb.DMatrix(X_train, label=y_train)\n",
    "xg_test = xgb.DMatrix(X_test, label=y_test)\n",
    "param = {}\n",
    "# I used gamma regression \n",
    "param['objective'] = 'reg:gamma'\n",
    "param['eta'] = 0.05\n",
    "param['max_depth'] =6\n",
    "param['silent'] = 0\n",
    "param['nthread'] = 4\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "num_round = 250\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.train(param, xg_train, num_round, watchlist)\n",
    "pred = bst.predict(xg_test)\n",
    "mean_absolute_error(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Out of the box XGboost: 0.9150395736506789 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling with Vader Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = pd.read_csv('vader.csv')\n",
    "vader.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader.drop('Additional_Number_of_Scoring', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = vader.pop('Reviewer_Score')\n",
    "X = vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Out of the box random forest with vader sentiment analysis:0.864468939011441"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdbr.fit(X_train, y_train)\n",
    "y_hat = rf.predict(X_test)\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Out of the box gradient boost with vader sentiment analysis: 0.864468939011441"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_train = xgb.DMatrix(X_train, label=y_train)\n",
    "xg_test = xgb.DMatrix(X_test, label=y_test)\n",
    "param = {}\n",
    "# I used gamma regression \n",
    "param['objective'] = 'reg:gamma'\n",
    "param['eta'] = 0.05\n",
    "param['max_depth'] =6\n",
    "param['silent'] = 0\n",
    "param['nthread'] = 4\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "num_round = 250\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist)\n",
    "pred = bst.predict(xg_test)\n",
    "mean_absolute_error(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, pred)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y, vader['Average_Score'])**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Out of the box XGboost with vader sentiment analysis:  0.8603997258088616"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vader slightly better, will proceed with vader sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Vader with count vector of top 300 words from each positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanest = pd.read_csv('cleanest.csv')\n",
    "count = CountVectorizer(max_features=300, stop_words='english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_pos = count.fit_transform(cleanest['Pos_Review_Clean'].values.astype('U'))\n",
    "pos_col_names = count.get_feature_names()\n",
    "fitted_pos = fitted_pos.todense()\n",
    "fitted_pos = pd.DataFrame(fitted_pos, columns = pos_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = vader.join(fitted_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_neg = count.fit_transform(cleanest['Neg_Review_Clean'].values.astype('U'))\n",
    "neg_col_names = count.get_feature_names()\n",
    "fitted_neg = fitted_neg.todense()\n",
    "fitted_neg = pd.DataFrame(fitted_neg, columns = neg_col_names)\n",
    "fitted_neg = fitted_neg.add_suffix('_neg')\n",
    "vader = vader.join(fitted_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader.to_csv('vadercountfinal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vadercountfinal = pd.read_csv('vadercountfinal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vadercountfinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = vadercountfinal.pop('Reviewer_Score')\n",
    "X = vadercountfinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(min_samples_leaf = 30,\n",
    "                          max_depth=25,\n",
    "                          max_features=10,\n",
    "                          n_estimators=500)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Out of the box random forest with vader and count vectorize: 0.9482405860580165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdbr = GradientBoostingRegressor(learning_rate=0.05,\n",
    "                                  loss='ls',\n",
    "                                 max_depth=15,\n",
    "                                  n_estimators=500,\n",
    "                                 min_samples_leaf=120,\n",
    "                                 max_features=10,\n",
    "                                  random_state=1)\n",
    "gdbr.fit(X_train, y_train)\n",
    "y_hat = gdbr.predict(X_test)\n",
    "mean_absolute_error(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Out of the box gradient boost with vader and count vectorizer: 0.7937548145846729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vadercountfinal = pd.read_csv('vadercountfinal.csv')\n",
    "X = vadercountfinal\n",
    "y = y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_train = xgb.DMatrix(X_train, label=y_train)\n",
    "xg_test = xgb.DMatrix(X_test, label=y_test)\n",
    "param = {}\n",
    "# I used gamma regression \n",
    "param['objective'] = 'reg:gamma'\n",
    "param['eta'] = 0.02\n",
    "param['max_depth'] = 10\n",
    "param['silent'] = 0\n",
    "param['nthread'] = 4\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "num_round = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = bst2.predict(xg_test)\n",
    "mean_absolute_error(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best tuned XGBoost: 0.7745135257798303"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(min_samples_leaf = 5,\n",
    "                          max_depth=30,\n",
    "                          max_features=60,\n",
    "                          n_estimators=1000)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best tuned random forest: .8199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdbr = GradientBoostingRegressor(learning_rate=0.01,\n",
    "                                  loss='ls',\n",
    "                                 max_depth=35,\n",
    "                                  n_estimators=1250,\n",
    "                                 min_samples_leaf=80,\n",
    "                                 max_features=70,\n",
    "                                  random_state=1)\n",
    "\n",
    "gdbr.fit(X_train, y_train)\n",
    "y_hat = gdbr.predict(X_test)\n",
    "mean_absolute_error(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best tuned gradient boosting regressor: 0.764065461073389"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_dict = bst.get_score(importance_type='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = feats_dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(feats_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphing Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = [0.918, 0.920, 0.916]\n",
    "textblob = [0.885, 0.874, 0.879]\n",
    "vadergraph = [0.864, 0.864, 0.860]\n",
    "vadercountvectorizer = [0.812, 0.764, 0.774]\n",
    "\n",
    "\n",
    "\n",
    "N = 3\n",
    "ind = np.array((0,1,2))\n",
    "width = 0.25\n",
    "fig= plt.figure(figsize=(12,6))\n",
    "rects1 = plt.bar(ind, original, 4/5*width, label='Original Dataset')\n",
    "rects2 = plt.bar(ind + width, textblob, 4/5*width,\n",
    "    label='With Textblob')\n",
    "rects3 = plt.bar(ind + 2*width, vadergraph, 4/5*width,\n",
    "    label='With Vader')\n",
    "rects4 = plt.bar(ind + 3*width, vadercountvectorizer, 4/5*width,\n",
    "    label='Vader CountVectorizer')\n",
    "\n",
    "\n",
    "\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Scores by dataset and model')\n",
    "plt.axis([-.25,3,0.7,0.95])\n",
    "plt.xticks(ind + 1.5*width, ('Random Forest', 'Gradient Boost', 'XGBoost'))\n",
    "plt.legend(loc='best')\n",
    "\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        h = rect.get_height()\n",
    "        plt.text(rect.get_x()+rect.get_width()/2., 1*h, h,\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "autolabel(rects3)\n",
    "autolabel(rects4)\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('Hotel-Reviews/images/modelcomparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a dictionary to find word occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words = {}\n",
    "for i in negative_reviews:\n",
    "    try:\n",
    "        for w in i.split():\n",
    "            if w not in negative_words.keys():\n",
    "                negative_words[w] = 0\n",
    "            negative_words[w] +=1\n",
    "    except AttributeError:\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_list = sorted(negative_words.values())[::-1][:30]\n",
    "for k,v in negative_words.items():\n",
    "    if v in top_list:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = {}\n",
    "for i in positive_reviews:\n",
    "    try:\n",
    "        for w in i.split():\n",
    "            if w not in positive_words.keys():\n",
    "                positive_words[w] = 0\n",
    "            positive_words[w] +=1\n",
    "    except AttributeError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_list = sorted(negative_words.values())[::-1][:30]\n",
    "for k,v in negative_words.items():\n",
    "    if v in top_list:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(max_features=1000, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = count.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_neg = count.fit_transform(neg_content.values.astype('U')) \n",
    "fitted_pos = count.fit_transform(pos_content.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=3, init='random', random_state=0)\n",
    "W = model.fit_transform(fitted_neg)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associated_pos = []\n",
    "for i in H2:\n",
    "    associated_pos.append((np.argsort(i)[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_pos = []\n",
    "for num, i in enumerate(associated_pos):\n",
    "    lst = []\n",
    "    for idx in i:\n",
    "        lst.append(words[idx])\n",
    "    topics_pos.append(lst)\n",
    "    print(f'topic{num+1} :', lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wc = WordCloud(background_color=\"white\", colormap=\"Dark2\",\n",
    "               max_font_size=150, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [40, 10]\n",
    "\n",
    "# Create subplots for each Topic\n",
    "for index, topic in enumerate(topics_pos):\n",
    "    wc = WordCloud(width = 1000, height = 500).generate(' '.join(topic))\n",
    "    \n",
    "    plt.subplot(2, 5, index+1)\n",
    "    plt.imshow(wc, interpolation=\"quadric\")\n",
    "    plt.axis(\"off\")\n",
    "    #plt.title(hand_labels[index])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color = 'white',\n",
    "        max_words = 25,\n",
    "        max_font_size = 30, \n",
    "        scale = 3,\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize = (10, 10))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize = 20)\n",
    "        fig.subplots_adjust(top = 2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "fig = show_wordcloud(topics_pos[0])\n",
    "fig.savefig('Hotel-Reviews/images/topic1.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = show_wordcloud(topics_pos[1])\n",
    "fig2.savefig('Hotel-Reviews/images/topic2.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3 = show_wordcloud(topics_pos[2])\n",
    "fig3.savefig('Hotel-Reviews/images/topic3.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(fitted_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapdf = data[['Hotel_Name', 'Average_Score', 'lat', 'lng']]\n",
    "mapdf = mapdf.groupby(['Hotel_Name']).agg({'Average_Score':'mean', 'lat':'mean', 'lng':'mean'})\n",
    "mapdf = mapdf.dropna()\n",
    "mapdf.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapdf['color'] = pd.cut(mapdf['Average_Score'], bins=[0,8.1,8.5,8.9,10], \n",
    "                              labels=['red', 'orange', 'blue', 'green'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "m = folium.Map(location=[52.360576, 4.915968])\n",
    "\n",
    "image_file='legend.png'\n",
    "\n",
    "FloatImage(image_file,bottom=5,left=5).add_to(m)\n",
    "\n",
    "for i in range(mapdf.shape[0]):\n",
    "    folium.CircleMarker([mapdf['lat'][i],mapdf['lng'][i]], radius=2,tooltip=f'{mapdf[\"Hotel_Name\"][i]} : {mapdf[\"Average_Score\"][i]:0.2}', color=mapdf['color'][i]).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.plugins import FloatImage\n",
    "\n",
    "\n",
    "import folium\n",
    "b = folium.Map(location=[52.360576, 4.915968])\n",
    "# convert to (n, 2) nd-array format for heatmap\n",
    "import folium.plugins as plugins\n",
    "lat = np.array(mapdf['lat'])\n",
    "lng = np.array(mapdf['lng'])\n",
    "coords = []\n",
    "for i,j in zip(lat,lng):\n",
    "    coords.append((i,j))\n",
    "hotels = np.array(coords)\n",
    "\n",
    "image_file='legend.png'\n",
    "\n",
    "FloatImage(image_file,bottom=5,left=5).add_to(b)\n",
    "\n",
    "# plot heatmap\n",
    "for i in range(mapdf.shape[0]):\n",
    "    folium.CircleMarker([mapdf['lat'][i],mapdf['lng'][i]], radius=2, tooltip=f'{mapdf[\"Hotel_Name\"][i]} : {mapdf[\"Average_Score\"][i]:0.2}', color=mapdf['color'][i]).add_to(b)\n",
    "\n",
    "\n",
    "b.add_children(plugins.HeatMap(hotels, radius=30))\n",
    "b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapdata = np.array(mapdf[['lat', 'lng','Average_Score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Vader Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = analyser.lexicon.keys()\n",
    "b = analyser.lexicon.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_lexicon = {k:v for k,v in zip(a,b)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = data['Negative_Review']\n",
    "neg_review_sentiment = []\n",
    "for i in neg:\n",
    "    snt = analyser.polarity_scores(i)\n",
    "    neg_review_sentiment.append(snt['compound'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = data['Positive_Review']\n",
    "pos_review_sentiment = []\n",
    "for i in pos:\n",
    "    snt = analyser.polarity_scores(i)\n",
    "    pos_review_sentiment.append(snt['compound'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vader.drop(['neg_review_sentiment', 'pos_review_sentiment'], axis=1, inplace=True)\n",
    "\n",
    "vader['pos_sent'] = pos_review_sentiment\n",
    "vader['neg_sent'] = neg_review_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader.drop(['Unnamed: 0', 'Unnamed: 0.1','Neg_Review_Clean', 'Pos_Review_Clean'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader.to_csv('vader.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader.drop('Additional_Number_of_Scoring', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_dict = {}\n",
    "for idx, i in enumerate(cleanest['Neg_Review_Clean']):\n",
    "    try:\n",
    "        for w in word_tokenize(i):\n",
    "            if w.isalpha():\n",
    "                w=w.lower()\n",
    "                if w not in neg_dict.keys():\n",
    "                    neg_dict[w] = 0\n",
    "                neg_dict[w] -=1\n",
    "    except TypeError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(neg_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = {}\n",
    "for i in cleanest['Pos_Review_Clean']:\n",
    "    try:\n",
    "        for w in word_tokenize(i):\n",
    "            w=w.lower()\n",
    "            if w.isalpha():\n",
    "                if w not in pos_dict.keys():\n",
    "                    pos_dict[w] = 0\n",
    "\n",
    "                pos_dict[w]+=1\n",
    "    except TypeError:\n",
    "        continue            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(pos_dict.values())[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pos_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(neg_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dict = neg_dict.copy()\n",
    "\n",
    "for i in pos_dict.keys():\n",
    "    if i not in merged_dict.keys():\n",
    "        merged_dict[i]=0\n",
    "    merged_dict[i] += pos_dict[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_neg = sorted(merged_dict.items(), key=(lambda item: item[1]))[:55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pos = sorted(merged_dict.items(), key=(lambda item: item[1]))[73480:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_pos[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = analyser.lexicon.keys()\n",
    "b = analyser.lexicon.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_lexicon = {k:v for k,v in zip(a,b)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_lexicon = []\n",
    "for i in top_pos:\n",
    "    if i[0] in hotel_lexicon.keys():\n",
    "        in_lexicon.append((i[0], hotel_lexicon[i[0]]))\n",
    "    else:\n",
    "        in_lexicon.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_in_lexicon = [i for i in in_lexicon if i[1]>4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nin_lexicon = []\n",
    "for i in top_neg:\n",
    "    if i[0] in hotel_lexicon.keys():\n",
    "        nin_lexicon.append((i[0], hotel_lexicon[i[0]]))\n",
    "    else:\n",
    "        nin_lexicon.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_in_lexicon = [i for i in nin_lexicon if i[1]<-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_in_lexicon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_pos_in_lexicon = [i[0] for i in pos_in_lexicon]\n",
    "w_neg_in_lexicon = [i[0] for i in neg_in_lexicon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w_neg_in_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "#vader update\n",
    "pos_words = {'large':1.5,\n",
    " 'minute':1,\n",
    " 'convenient':1.5,\n",
    " 'walking':1,\n",
    " 'near':1,\n",
    " 'central':1,\n",
    " 'view':1,\n",
    " 'walk':1,\n",
    " 'spacious':2,\n",
    " 'modern':2,\n",
    " 'everything':1,\n",
    " 'quiet':1,\n",
    " 'comfy':1,\n",
    " 'close':1,\n",
    " 'location':1.5}\n",
    "\n",
    "neg_words = {\n",
    " 'i':-.05\n",
    " 'small': -2,\n",
    " 'little':-2,\n",
    " 'expensive': -2,\n",
    " 'work':-1,\n",
    " 'not':-1,\n",
    " 'air':-1,\n",
    " 'noise': -2,\n",
    " 'told':-0.5,\n",
    " 'bathroom':-1,\n",
    " 'water':-1,\n",
    " 'booking': -1,\n",
    " 'hot':-1,\n",
    " 'shower':-2,\n",
    " 'cold': -3,\n",
    " 'price':-1,\n",
    " 'slow': -2,\n",
    " 'booked':-1,\n",
    " 'old':-1,\n",
    " 'paid':-0.5,\n",
    " 'toilet':-1,\n",
    " 'tiny':-2,\n",
    " 'working':-1}\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "analyser.lexicon.update(pos_words)\n",
    "analyser.lexicon.update(neg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = pd.read_csv('vader.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Negative_Review\"] = data[\"Negative_Review\"].apply(lambda x: str(x).replace(\"No Negative\", \"\"))\n",
    "data[\"Positive_Review\"] = data[\"Positive_Review\"].apply(lambda x: str(x).replace(\"No Positive\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = data['Positive_Review']\n",
    "pos_review_sentiment = []\n",
    "for i in pos:\n",
    "    snt = analyser.polarity_scores(i)\n",
    "    pos_review_sentiment.append(snt['compound'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = data['Negative_Review']\n",
    "neg_review_sentiment = []\n",
    "for i in neg:\n",
    "    snt = analyser.polarity_scores(i)\n",
    "    neg_review_sentiment.append(snt['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader3 = vader2.copy()\n",
    "vader3['new_pos_sent'] = pos_review_sentiment\n",
    "vader3['New_neg_sent'] = neg_review_sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader3.to_csv('vader3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader2.to_csv('vader2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader2 = pd.read_csv('vader2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader2.drop(['pos_sent', 'neg_sent'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edited_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted(edited_dict.items(), key=(lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_dict = {key:val for key, val in merged_dict.items() if key in edited_dict.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [('Location', 182356),\n",
    " ('Staff', 156235),\n",
    " ('Good', 91507),\n",
    " ('Friendly', 80715),\n",
    " ('Helpful', 71799),\n",
    " ('Excellent', 59915),\n",
    " ('Nice', 59561),\n",
    " ('Clean', 58436),\n",
    " ('Comfortable', 54418),\n",
    " ('Hotel', 49731)]\n",
    "words = []\n",
    "vals = []\n",
    "for i in lst:\n",
    "    words.append(i[0])\n",
    "    vals.append(i[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = pd.DataFrame([words,vals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words=pos_words.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot positive word occurrences\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.bar(pos_words[0], pos_words[1], color='green')\n",
    "ax.xaxis.set_tick_params(labelsize=16, rotation=45, )\n",
    "ax.set_xlabel('Positive Words', fontsize=18)\n",
    "ax.set_ylabel('Relative Occurrences', fontsize=18)\n",
    "fig.suptitle('Top 10 Positive Words', fontsize=20);\n",
    "plt.savefig('Hotel-Reviews/images/poswords.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neglst = [ \n",
    "('I', 54831),\n",
    " ('Small', 38226),\n",
    " ('Nothing', 32126),\n",
    " ('Room', 29869),\n",
    " ('Bit', 23675),\n",
    " ('Could', 22721),\n",
    " ('Poor', 15300),\n",
    " ('Little', 15107),\n",
    " ('Expensive', 14225),\n",
    " ('Noisy', 12803)\n",
    "]\n",
    "words = []\n",
    "vals = []\n",
    "for i in neglst:\n",
    "    words.append(i[0])\n",
    "    vals.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words = pd.DataFrame([words,vals])\n",
    "neg_words=neg_words.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot negative word occurrences\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.bar(neg_words[0], neg_words[1], color='red')\n",
    "ax.xaxis.set_tick_params(labelsize=16, rotation=45)\n",
    "ax.set_xlabel('Negative Words', fontsize=18)\n",
    "ax.set_ylabel('Relative Occurrences', fontsize=18)\n",
    "fig.suptitle('Top 10 Negative Words', fontsize=20);\n",
    "plt.savefig('Hotel-Reviews/images/negwords.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurelist = [('fabulous', 0.29964397592170217), ('room', 0.30194291249199645), ('Review_Total_Positive_Word_Counts', 0.3052570176688559), ('view_neg', 0.3065943580036194), ('customer_neg', 0.309148504285848), ('like', 0.3097000750242623), ('cleaned_neg', 0.3138296421587767), ('far_neg', 0.31415213928598024), ('experience_neg', 0.3160898314851383), ('upgraded', 0.31808503933448273), ('uncomfortable_neg', 0.3181067243519525), ('amazing', 0.32032346693149794), ('positive', 0.3268533664683133), ('friendly_neg', 0.3279204781542303), ('star', 0.3316088215976614), ('money', 0.33323600792235114), ('furniture_neg', 0.33556941761333337), ('thing_neg', 0.3359991773269183), ('basic_neg', 0.3371390389734257), ('worth_neg', 0.34183704961622896), ('bad_neg', 0.35738071688622886), ('excellent', 0.3594808207557529), ('service', 0.359927338350747), ('good', 0.36450212979652796), ('value_neg', 0.36939321877731096), ('Average_Score', 0.37155553164158), ('maybe_neg', 0.3742655712045161), ('slightly_neg', 0.3756614262933333), ('problem_neg', 0.37852534112012903), ('negative_neg', 0.3795259726073531), ('liked_neg', 0.3798875420290196), ('loved', 0.38373564215679484), (' Leisure trip ', 0.3845988736897397), ('thing', 0.3854006269861976), ('tiny_neg', 0.3855309579764056), ('smell_neg', 0.39104875876111), ('comfort', 0.396663971459846), ('service_neg', 0.40567563202398277), ('poor_neg', 0.40716188873969195), ('location', 0.4366442383734017), ('exceptional', 0.44605379501967746), ('fantastic', 0.4506617635248613), ('manager_neg', 0.4539769613296106), ('dated_neg', 0.4552046506772193), ('little_neg', 0.4669997007938732), ('bed_neg', 0.48724146197141455), ('location_neg', 0.5196409792334291), ('terrible_neg', 0.5253355840173783), ('ok', 0.533507276280509), ('old_neg', 0.5588221246639997), ('rude_neg', 0.5628752955510673), ('bit_neg', 0.5665008095155175), ('overpriced_neg', 0.5972159823682486), ('staff', 0.6016395407762328), ('hotel_neg', 0.6126645258793607), ('clean_neg', 0.6272994880442819), ('rooms_neg', 0.6394214532205174), ('Review_Total_Negative_Word_Counts', 0.643662427137203), ('star_neg', 0.6624058946175011), ('money_neg', 0.6654582424585267), ('cleanliness', 0.702056108507394), ('perfect_neg', 0.7507870704583097), ('fault_neg', 0.7786412114249133), ('dirty_neg', 0.9736228654579733), ('staff_neg', 1.1175715739400547), ('new_pos_sent', 1.197688207777768), ('New_neg_sent', 1.3588873831895074), ('room_neg', 3.213848362698663)][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best and Worst Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = pd.read_csv('vader.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Positive_Review'][vader.sort_values('vader_pos_sent', ascending=False).index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Negative_Review'][vader.sort_values('vader_neg_sent').index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
